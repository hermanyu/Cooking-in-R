[["modeling.html", "Chapter 9 Modeling 9.1 A concrete example 9.2 Statistical decision theory 9.3 Building the model", " Chapter 9 Modeling Data usually comes with multiple variables and analyzing the covariation between these variables helps us understand the data. Model building is one method for analyzing this covariation. The model building process can be quite open-ended and freeform with an infinite number of design choices to be explored. We should formalize a basic framework to systematically study the model buidling process. We start with an example of a situation highlighting the core elements of model building, then generalize this example to a more formal mathematical setting. 9.1 A concrete example As a concrete example, let’s suppose we wanted to study the relationship between a company’s marketing spend and their revenue over a given month. We can represent this scenario by letting \\(X=(X_1,\\ldots,X_k)\\) represent the amount of marketing spend across different marketing channels by a company. So we might have \\(X_1\\) is TV ad spend, \\(X_2\\) is social media spend, \\(X_3\\) paid search spend, etc. We can let \\(Y\\) be the revenue earned by that company. It seems reasonable to believe that the more money a company spends on marketing, the more revenue they earn from sales. We want determine the exact nature of this relationship, so we can conceptualize revenue as a function of marketing spend: \\(Y=f(X)\\). The issue is that revenue figures are inherently random: some people will choose to buy (or not buy) a product on a whim. So it is unreasonable to expect that marketing spend \\(X\\) can fully predict the exact value of revenue \\(Y\\). Just ask ourselves: if two companies have the same exact marketing spend values \\(X=x\\), can we guarantee they will always have the same revenue figures? The answer is probably “no!” So if we can’t fully predict \\(Y\\), is there a related quantity we can hope to predict? It seems more reasonable to try and predict the expected value of \\(Y\\) at given levels of marketing spend \\(X=x\\), which we call the conditional mean \\(E(Y|X=x)\\). Another quantity we can try to predict is the conditional median \\(\\text{median}(Y|X=x)\\). So we can attempt to craft a function \\(f\\) where \\(f(x)=E(Y|X=x)\\) for every value of \\(x\\). This would allow us to predict a “best guess” estimate of revenue \\(Y\\) at each possible spend level \\(X=x\\). We want to explicity determine this function \\(f\\) but this is not possible. In order to determine \\(f\\) completely, we would need the \\((X,Y)\\) data for every company that ever existed and every company that ever will exist! Let’s call this mythological data set containing every company the population. Since we can’t obtain the population data set, we can’t compute \\(f\\) directly. Instead, we try to approximate it using an estimate \\(\\hat{f}\\). To construct the estimate \\(\\hat{f}\\), what we can do is take a sample of data points \\(\\{(X^{(i)},Y^{(i)})\\}_{i=1}^n\\) with each \\((X^{(i)}, Y^{(i)})\\) representing a marketing spend value and a revenue figure. In our example, this might the marketing spend and revenue values from the same company each month over a period of 10 years. What we want to do is use the sample data as a proxy for the population data. How we build this is the main topic of discussion from this chapter onward. 9.2 Statistical decision theory We take the previous example and formalize it. Let \\(X=(X_1,\\ldots,X_k)\\) be a vector of \\(k\\) variables and \\(Y\\) a random variable which “depends” on \\(X\\). We want to study the exact nature of \\(Y\\)’s dependency on \\(X\\). One approach is build a function \\(f\\) which can construct \\(Y\\) out of the data given by \\(X\\). In other words, we seek a function \\(f\\) such that \\(Y=f(X)\\). Unfortunately, since \\(Y\\) is random, a function like this doesn’t exist. So we adjust our requirements and instead ask for an \\(f\\) which predicts \\(Y\\) as best as possible. What do we mean by “best possible” prediction? Well there are multiple valid answers. One idea is to measure \\(f\\)’s predictive power by measuring \\(f\\)’s prediction error. A loss function \\(L\\) is a functional \\(L:\\text{Maps}(\\mathbb{R}^n,\\mathbb{R})\\to \\mathbb{R}\\) where \\(L(f)\\) represents the prediction error of \\(f\\). One such loss function is he expected squared error (ESE) given by: \\[ \\begin{align*} ESE(f) &amp;= E(Y-f(X))^2 \\\\ &amp;=\\int(y-f(x))^2Pr(dx,dy) \\\\ &amp;=E_XE_{Y|X}\\left([Y-f(X)]^2|X\\right) \\end{align*} \\] This quantity \\(ESE\\) measures the average squared error made by \\(f\\) when predicting the values of \\(Y\\). Since we want \\(f(X)\\) to be the “best” possible prediction of \\(Y\\), \\(f(X)\\) should minimize the \\(ESE\\) quantity: \\[ f(x) = \\min_{c} E_XE_{Y|X}\\left([Y-c]^2|X=x\\right) \\] Think about what this equation is saying: for a fixed value \\(X=x\\), the quantity \\(f(x)\\) should minimize the expected squared error \\(E(Y-f(x))^2\\) in \\(Y\\). We actually know exactly what quantity \\(f(x)\\) is, it’s just the (conditional) mean! \\[ f(X) = E(Y|X=x) \\] To summarize: \\(f(x)\\) should best predict \\(Y\\) given \\(X=x\\). If we use \\(ESE\\) as our standard for predictive power, then conditional mean \\(f(X) = E(Y|X=x)\\) is the quantity that best predicts \\(Y\\) given \\(X=x\\). Aside: This actually makes intuitive sense. In the absence of any information, the mean is always the best predictor in that it minimizes squared error. If we have \\(Y\\) is dependent on \\(X\\), then once we control for \\(X=x\\), our best prediction for \\(Y\\) should be the mean again \\(E(Y|X=x)\\). Of course, \\(ESE\\) is just one possible measure of prediction error. Another possible measure of prediction error is expected absolute error (EAE): \\[ EAE = E\\big(\\,|Y-f(X)| \\,\\big) \\] The \\(f\\) which minimizes this quantity is the conditional median: \\[ f(X) = \\text{median}(Y|X=x) \\] All-in-all: we want to study the dependency of \\(Y\\) on \\(X\\) by building a function \\(f\\) which best predicts \\(Y\\). However, there are many different ways to quantify what “best” actually means and the quantity \\(f(X)\\) can represent different statistical parameters. In practice, the squared error ESE is usually preferred because of analytical properties like differentiability. That being said, the EAE is more robust to outliers. 9.3 Building the model The function \\(f(x)=E(Y|X=x)\\) offers the best predictions (in the sense of squared loss). We desire to explicitly know \\(f\\), but this requires knowing the entire distribution \\(Pr(X,Y)\\) which is usually impossible to attain. The next best alternative is find an approximation \\(\\hat{f}\\) for the ground truth \\(f\\). The idea is to sample a set of data points \\(\\{(X^{(i)},Y^{(i)})\\}_{i=1}^n\\) from the distribution \\(Pr(X,Y)\\) and use the sample as a proxy for the distribution. That is, we wish to estimate \\(f(x) = E(Y|X=x)\\) from the sample. In a perfect world, we would have many data points \\((x,Y)\\) for each fixed value of \\(X=x\\) and we could average the \\(Y\\) values across fixed \\(X=x\\). However, this is rarely the case and downright impossible for continuous \\(X\\). 9.3.1 \\(k\\)-nearest neighbors One approach: if we don’t have enough data points of a fixed \\(x\\) value, why don’t we use data points with nearby \\(x\\) values? For a given \\(x\\) value, let \\(N_k(x) = \\{(X^{(i_1)}, Y^{(i_1)}),\\ldots, (X^{(i_k)},Y^{(i_k)})\\}\\) be the \\(k\\) data points with \\(X\\) values closest to \\(x\\). We call \\(N_k(x)\\) the \\(k\\)-nearest neighbors of \\(x\\). Then we make the approximation: \\[ \\begin{align*} f(X) &amp;= E(Y|X=x) \\\\ &amp; \\approx E\\left[N_k(x)\\right] \\\\ &amp;= \\frac{1}{k}\\sum_{j=1}^k Y^{(i_j)} \\end{align*} \\] Explicitly, the approximation we make is \\(\\hat{f}=E[N_k(x)]\\) and this model for \\(f\\) is called the \\(k\\)-nearest neighbors model. 9.3.2 Decision trees The \\(k\\)-nearest neighbors approach motivates another approach to solving the problem. Since we want to approximate \\(f(X) = E(Y|X=x)\\), we can try to group similar data points with similar \\(X=x\\) values together and take the group average. \\(k\\)-nearest neighbors did this by defining “similar” to mean closest data points in Euclidean distance. What if we had a different method for defining “similar”? Another approach is to identify clusters of data points together making splits along the \\(X_1,\\ldots, X_m\\) dimensions. In other words, we can draw rectangles in the \\(X_1,\\ldots,X_m\\) to group together data points in \\(X\\) space by making splits along the coordinate \\(X_i\\) axis. The optimal splits are chosen by greatest reduction to the variance in the \\(Y^{(i)}\\). The exact algorithm will be discussed later, but the main point is we are grouping together points in \\(X\\)-space by optimizing some criterion. After the groupings are computed, we average the \\(Y^{(i)}\\) values in each cluster to get the estimate for \\(E(Y|X=x)\\) we desire. This type of model is called a decision tree. 9.3.3 Model-based approach Another way to approximate \\(f(x)\\) is to find a \\(\\hat{f}\\) which minimizes the prediction error over the sample. Recall that prediction error over the population is the expected squared error: \\[ESE(f) = E(Y-f(x))^2\\] The sample analogue of ESE is the mean squared error (MSE) given by: \\[ \\begin{align*} MSE(f) &amp;= E(Y^{(i)}-f(X^{(i)}))^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n\\left(Y^{(i)}-f(X^{(i)})\\right)^2 \\end{align*} \\] Since \\(f\\) minimizes the ESE over the population, a good approximation for \\(f\\) might be a function \\(\\hat{f}\\) which minimizes the MSE over the sample. There is an issue with this approach: since we only have finitely many sample points, there are infinitely many functions which minimize MSE. Think about it: there exists infinitely many functions which cross through all the sample points, i.e. \\(\\hat{f}(X^{(i)})=Y^{(i)}\\) for all \\(i\\). Therefore, there exists infinitely many \\(\\hat{f}\\) for which \\(MSE=0\\). In order to obtain a unique solution for \\(\\hat{f}\\), we must impose a condition on the types of \\(\\hat{f}\\) we wish to permit. For example: Linear models: we assume \\(\\hat{f}\\) is linear \\[\\hat{f}(X) = X^T\\beta = \\beta_0+\\beta_1X_1+\\ldots+\\beta_mX_m\\] Polynomial models: we assume \\(\\hat{f}\\) is a polynomial in \\(X_1,\\ldots,X_n\\) Generalized linear models: we assume \\(\\hat{f}\\) is linear up to a link function \\[\\hat{f}(X) = g\\left(X^T\\beta\\right)\\] Generalized additive models: we assume \\(\\hat{f}\\) is a linear combination of smooth functions up to a link function \\[\\hat{f}(X) = g\\left(\\beta_0+f_1(X_1)+\\ldots+f_m(X_m)\\right)\\] Assuming \\(\\hat{f}\\) must be of a specific form is called a model-based approach because we are modeling our \\(\\hat{f}\\) after a specific function class. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
