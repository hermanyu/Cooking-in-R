[["modeling.html", "Chapter 9 Modeling 9.1 A concrete example 9.2 Statistical decision theory 9.3 Building the model 9.4 Minimizing loss functions 9.5 Maximizing likelihood 9.6 The additive error model and the bias-variance tradeoff", " Chapter 9 Modeling Data usually comes with multiple variables and analyzing the covariation between these variables helps us understand the data. Model building is one method for analyzing this covariation. The model building process can be quite open-ended and freeform with an infinite number of design choices to be explored. We should formalize a basic framework to systematically study the model buidling process. We start with an example of a situation highlighting the core elements of model building, then generalize this example to a more formal mathematical setting. 9.1 A concrete example As a concrete example, let’s suppose we wanted to study the relationship between a company’s marketing spend and their revenue over a given month. We can represent this scenario by letting \\(X=(X_1,\\ldots,X_k)\\) represent the amount of marketing spend across different marketing channels by a company. So we might have \\(X_1\\) is TV ad spend, \\(X_2\\) is social media spend, \\(X_3\\) paid search spend, etc. We can let \\(Y\\) be the revenue earned by that company. It seems reasonable to believe that the more money a company spends on marketing, the more revenue they earn from sales. We want determine the exact nature of this relationship, so we can conceptualize revenue as a function of marketing spend: \\(Y=f(X)\\). The issue is that revenue figures are inherently random: some people will choose to buy (or not buy) a product on a whim. So it is unreasonable to expect that marketing spend \\(X\\) can fully predict the exact value of revenue \\(Y\\). Just ask ourselves: if two companies have the same exact marketing spend values \\(X=x\\), can we guarantee they will always have the same revenue figures? The answer is probably “no!” So if we can’t fully predict \\(Y\\), is there a related quantity we can hope to predict? It seems more reasonable to try and predict the expected value of \\(Y\\) at given levels of marketing spend \\(X=x\\), which we call the conditional mean \\(E(Y|X=x)\\). Another quantity we can try to predict is the conditional median \\(\\text{median}(Y|X=x)\\). So we can attempt to craft a function \\(f\\) where \\(f(x)=E(Y|X=x)\\) for every value of \\(x\\). This would allow us to predict a “best guess” estimate of revenue \\(Y\\) at each possible spend level \\(X=x\\). We want to explicity determine this function \\(f\\) but this is not possible. In order to determine \\(f\\) completely, we would need the \\((X,Y)\\) data for every company that ever existed and every company that ever will exist! Let’s call this mythological data set containing every company the population. Since we can’t obtain the population data set, we can’t compute \\(f\\) directly. Instead, we try to approximate it using an estimate \\(\\hat{f}\\). To construct the estimate \\(\\hat{f}\\), what we can do is take a sample of data points \\(\\{(X^{(i)},Y^{(i)})\\}_{i=1}^n\\) with each \\((X^{(i)}, Y^{(i)})\\) representing a marketing spend value and a revenue figure. In our example, this might the marketing spend and revenue values from the same company each month over a period of 10 years. What we want to do is use the sample data as a proxy for the population data. How we build this is the main topic of discussion from this chapter onward. 9.2 Statistical decision theory We take the previous example and formalize it. Let \\(X=(X_1,\\ldots,X_k)\\) be a vector of \\(k\\) variables and \\(Y\\) a random variable which “depends” on \\(X\\). We want to study the exact nature of \\(Y\\)’s dependency on \\(X\\). One approach is build a function \\(f\\) which can construct \\(Y\\) out of the data given by \\(X\\). In other words, we seek a function \\(f\\) such that \\(Y=f(X)\\). Unfortunately, since \\(Y\\) is random, a function like this doesn’t exist. So we adjust our requirements and instead ask for an \\(f\\) which predicts \\(Y\\) as best as possible. What do we mean by “best possible” prediction? Well there are multiple valid answers. One idea is to measure \\(f\\)’s predictive power by measuring \\(f\\)’s prediction error. A loss function \\(L\\) is a functional \\(L:\\text{Maps}(\\mathbb{R}^n,\\mathbb{R})\\to \\mathbb{R}\\) where \\(L(f)\\) represents the prediction error of \\(f\\). One such loss function is he expected squared error (ESE) given by: \\[ \\begin{align*} ESE(f) &amp;= E(Y-f(X))^2 \\\\ &amp;=\\int(y-f(x))^2Pr(dx,dy) \\\\ &amp;=E_XE_{Y|X}\\left([Y-f(X)]^2|X\\right) \\end{align*} \\] This quantity \\(ESE\\) measures the average squared error made by \\(f\\) when predicting the values of \\(Y\\). Since we want \\(f(X)\\) to be the “best” possible prediction of \\(Y\\), \\(f(X)\\) should minimize the \\(ESE\\) quantity: \\[ f(x) = \\min_{c} E_XE_{Y|X}\\left([Y-c]^2|X=x\\right) \\] Think about what this equation is saying: for a fixed value \\(X=x\\), the quantity \\(f(x)\\) should minimize the expected squared error \\(E(Y-f(x))^2\\) in \\(Y\\). We actually know exactly what quantity \\(f(x)\\) is, it’s just the (conditional) mean! \\[ f(X) = E(Y|X=x) \\] To summarize: \\(f(x)\\) should best predict \\(Y\\) given \\(X=x\\). If we use \\(ESE\\) as our standard for predictive power, then conditional mean \\(f(X) = E(Y|X=x)\\) is the quantity that best predicts \\(Y\\) given \\(X=x\\). Aside: This actually makes intuitive sense. In the absence of any information, the mean is always the best predictor in that it minimizes squared error. If we have \\(Y\\) is dependent on \\(X\\), then once we control for \\(X=x\\), our best prediction for \\(Y\\) should be the mean again \\(E(Y|X=x)\\). Of course, \\(ESE\\) is just one possible measure of prediction error. Another possible measure of prediction error is expected absolute error (EAE): \\[ EAE = E\\big(\\,|Y-f(X)| \\,\\big) \\] The \\(f\\) which minimizes this quantity is the conditional median: \\[ f(X) = \\text{median}(Y|X=x) \\] All-in-all: we want to study the dependency of \\(Y\\) on \\(X\\) by building a function \\(f\\) which best predicts \\(Y\\). However, there are many different ways to quantify what “best” actually means and the quantity \\(f(X)\\) can represent different statistical parameters. In practice, the squared error ESE is usually preferred because of analytical properties like differentiability. That being said, the EAE is more robust to outliers. 9.3 Building the model The function \\(f(x)=E(Y|X=x)\\) offers the best predictions (in the sense of squared loss). We desire to explicitly know \\(f\\), but this requires knowing the entire distribution \\(Pr(X,Y)\\) which is usually impossible to attain. The next best alternative is find an approximation \\(\\hat{f}\\) for the ground truth \\(f\\). The idea is to sample a set of data points \\(\\{(X^{(i)},Y^{(i)})\\}_{i=1}^n\\) from the distribution \\(Pr(X,Y)\\) and use the sample as a proxy for the distribution. That is, we wish to estimate \\(f(x) = E(Y|X=x)\\) from the sample. In a perfect world, we would have many data points \\((x,Y)\\) for each fixed value of \\(X=x\\) and we could average the \\(Y\\) values across fixed \\(X=x\\). However, this is rarely the case and downright impossible for continuous \\(X\\). 9.3.1 \\(k\\)-nearest neighbors One approach: if we don’t have enough data points of a fixed \\(x\\) value, why don’t we use data points with nearby \\(x\\) values? For a given \\(x\\) value, let \\(N_k(x) = \\{(X^{(i_1)}, Y^{(i_1)}),\\ldots, (X^{(i_k)},Y^{(i_k)})\\}\\) be the \\(k\\) data points with \\(X\\) values closest to \\(x\\). We call \\(N_k(x)\\) the \\(k\\)-nearest neighbors of \\(x\\). Then we make the approximation: \\[ \\begin{align*} f(X) &amp;= E(Y|X=x) \\\\ &amp; \\approx E\\left[N_k(x)\\right] \\\\ &amp;= \\frac{1}{k}\\sum_{j=1}^k Y^{(i_j)} \\end{align*} \\] Explicitly, the approximation we make is \\(\\hat{f}=E[N_k(x)]\\) and this model for \\(f\\) is called the \\(k\\)-nearest neighbors model. 9.3.2 Decision trees The \\(k\\)-nearest neighbors approach motivates another approach to solving the problem. Since we want to approximate \\(f(X) = E(Y|X=x)\\), we can try to group similar data points with similar \\(X=x\\) values together and take the group average. \\(k\\)-nearest neighbors did this by defining “similar” to mean closest data points in Euclidean distance. What if we had a different method for defining “similar”? Another approach is to identify clusters of data points together making splits along the \\(X_1,\\ldots, X_m\\) dimensions. In other words, we can draw rectangles in the \\(X_1,\\ldots,X_m\\) to group together data points in \\(X\\) space by making splits along the coordinate \\(X_i\\) axis. The optimal splits are chosen by greatest reduction to the variance in the \\(Y^{(i)}\\). The exact algorithm will be discussed later, but the main point is we are grouping together points in \\(X\\)-space by optimizing some criterion. After the groupings are computed, we average the \\(Y^{(i)}\\) values in each cluster to get the estimate for \\(E(Y|X=x)\\) we desire. This type of model is called a decision tree. 9.3.3 Model-based approach Another way to approximate \\(f(x)\\) is to find a \\(\\hat{f}\\) which minimizes the prediction error over the sample. Recall that prediction error over the population is the expected squared error: \\[ESE(f) = E(Y-f(x))^2\\] The sample analogue of ESE is the mean squared error (MSE) given by: \\[ \\begin{align*} MSE(f) &amp;= E(Y^{(i)}-f(X^{(i)}))^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n\\left(Y^{(i)}-f(X^{(i)})\\right)^2 \\end{align*} \\] Since \\(f\\) minimizes the ESE over the population, a good approximation for \\(f\\) might be a function \\(\\hat{f}\\) which minimizes the MSE over the sample. There is an issue with this approach: since we only have finitely many sample points, there are infinitely many functions which minimize MSE. Think about it: there exists infinitely many functions which cross through all the sample points, i.e. \\(\\hat{f}(X^{(i)})=Y^{(i)}\\) for all \\(i\\). Therefore, there exists infinitely many \\(\\hat{f}\\) for which \\(MSE=0\\). In order to obtain a unique solution for \\(\\hat{f}\\), we must impose a condition on the form of \\(f\\) and \\(\\hat{f}\\). For example: Linear models: we assume \\(f\\) is linear \\[f(X) = X^T\\theta = \\theta_0+\\theta_1X_1+\\ldots+\\theta_mX_m\\] Polynomial models: we assume \\(f\\) is a polynomial in \\(X_1,\\ldots,X_n\\) Generalized linear models: we assume \\(f\\) is linear up to a link function \\[f(X) = g\\left(X^T\\beta\\right)\\] Generalized additive models: we assume \\(f\\) is a linear combination of smooth functions up to a link function \\[f(X) = g\\left(\\theta_0+f_1(X_1)+\\ldots+f_m(X_m)\\right)\\] Assuming \\(f\\) must be of a specific form is called a model-based approach because we are modeling our \\(f\\) after a specific function class. Once we decide on a specific parametric form for \\(f_{\\theta}\\), we can set about finding an estimate \\(\\hat{f}\\) by approximating the parameters \\(\\theta\\). Again, we want our estimate \\(\\hat{f}\\) to minimize \\(MSE\\) and this criterion is how we get estimates for the \\(\\theta\\) values. 9.4 Minimizing loss functions Let \\(\\hat{f}(x) = \\hat{f}_{\\theta}(x)\\) be a function parameterized by a set of values \\(\\theta = (\\theta_1,\\ldots,\\theta_m)\\). For example if \\(\\hat{f}(X) = X^T\\beta\\) is linear, then \\(\\theta=\\beta\\). The objective is to find a specific \\(\\theta\\) such that the quantity: \\[MSE(\\theta) = \\frac{1}{n}\\sum_{i=1}^n\\left(Y^{(i)}-\\hat{f}_{\\theta}(X^{(i)})\\right)^2\\] is minimized. Notice that we are treating \\(MSE\\) as a function of \\(\\theta\\). There are 2 ways we can attempt to minimize the \\(MSE\\) quantity: Analytic solution: do some calculus and clever linear algebra to solve for \\(\\theta\\) exactly. Gradient descent: do an iterative computation that inches closer and closer to a solution. The first approach is generally only possible if \\(\\hat{f}_{\\theta}\\) has a relatively simple form, e.g. a linear model. The second approach can always be applied, but can fail to converge to an optimal solution if \\(MSE(\\hat{f}_{\\theta})\\) isn’t a convex function. The concept of gradient descent actually follows a very simple calculus principle: the gradient points in the direction of steepest increase. Therefore, if we want to minimize a loss function \\(L(\\theta)\\), we can start with a random guess \\(\\theta\\) and update our guess by moving in the opposite direction of the gradient: \\[\\theta := \\theta - \\frac{dL}{d\\theta}\\] After repeating this procedure, the hope is that we eventually arrive at a \\(\\theta\\) with an acceptably low loss value. As mentioned above, the main theoretical challenge for gradient descent is when the loss function \\(L\\) is not convex, i.e. when \\(L\\) has multiple local minimums. There is also a number of computational challenges. Firstly, when the function \\(f_{\\theta}\\) is sufficiently complicated, the gradient \\(\\frac{dL}{d\\theta}\\) can be extremely hard to compute. In these situations, it is often necessary to find a clever algorithm to compute the gradient. For example, neural networks have extremely complicated \\(f_{\\theta}\\) and the clever algorithm used to compute the gradient \\(\\frac{dL}{d\\theta}\\) is called back propagation. Secondly, gradient descent can become unstable if the step size along the gradient direction is too large. The naive solution to this problem is to take tiny steps during the descent, but this can lead to a very slow run time. A more sophisticated solution is to perform a line search for the optimal step size at each descent step. Thirdly, gradient descent be quite inefficient even with an optimal step size. This is because the gradient can zigzag back-and-forth as it winds its way down to the minimum. For this reason, the gradient descent algorithm is often adjusted to provide a more “direct” approach towards the minimum. One such adjustment is conjugate gradient descent in which orthonormal basis vectors are used to adjust the gradient direction. Another approach is the Newton - Raphson method which attempts to solve the equation \\(\\frac{dL}{d\\theta} = 0\\) via an iterative procedure \\[\\theta:=\\theta-\\frac{L&#39;(\\theta)}{L&#39;&#39;(\\theta)}\\] (This is the same algorithm in Calculus 1 where students are asked to find the roots of a function \\(f(x)\\) using tangent line approximations.) 9.5 Maximizing likelihood Minimizing lost functions isn’t the only game in town; we can also do maximum likelihood estimates. We have \\(f(x) = E(Y|X=x)\\) is the “best possible prediction” for \\(Y\\) using the information given by \\(X=x\\). Instead of trying to estimate the conditional mean \\(E(Y|X=x)\\) directly, we instead do something more ambitious and try to estimate the entire conditional distribution \\(Pr(Y|X=x)\\). Previously, we assumed that \\(E(Y|X=x) = f_{\\theta}(x)\\) had some specific model form, e.g. \\(f_{\\theta}=X^T\\theta\\). Now, we assume instead that the conditional distribution \\(Pr(Y|X=x, \\theta)\\) has a specific form, e.g. \\(Pr(Y|X=x) \\sim N(x^T\\theta, \\sigma^2)\\). Finding a good approximation for \\(Pr(Y|X=x)\\) now reduces to finding a good estimate for \\(\\theta\\). For any given value \\(\\hat{\\theta}\\), define the likelihood of \\(\\hat{\\theta}\\) given \\((X,Y)\\) as the quantity: \\[l(\\hat{\\theta} | X,Y) = Pr(Y|X,\\hat{\\theta})\\] In other words, the likelihood of \\(\\hat{\\theta}\\) is the probability of observing the values \\((X,Y)\\) if \\(\\hat{\\theta}\\) was the actual parameter of the distribution \\(Pr(Y|X)\\). The principle of maximum likelihood espouses that the \\(\\hat{\\theta}\\) which best estimates \\(\\theta\\) is the one with the greatest likelihood. Specifically: \\[ \\begin{align*} \\hat{\\theta} &amp;:= \\max_{\\hat{\\theta}} \\,\\, l(\\hat{\\theta}|X^{(1)},\\ldots,X^{(n)},Y^{(1)},\\ldots,Y^{(n)}) \\\\ &amp;= \\max_{\\hat{\\theta}} \\,\\,Pr(Y^{(1)},\\ldots,Y^{(n)}|X^{(1)},\\ldots,X^{(n)},\\hat{\\theta})\\\\ &amp;= \\max_{\\hat{\\theta}} \\,\\,\\prod_{i=1}^n Pr(Y^{(i)}|X^{(i)},\\hat{\\theta}) \\end{align*} \\] In other words, \\(\\hat{\\theta}\\) should be the parameter(s) for which the observed sample is most probable. Note that the we are able to split the probability into a product because we assume the each data point is drawn independently during the sampling process. We can convert the product into a sum by feeding the likelihood \\(l(\\hat{\\theta})\\) into the \\(\\log()\\) function. If we do this, we get: \\[ \\begin{align*} \\log(l(\\hat{\\theta})) &amp;= \\log\\left(\\prod_{i=1}^n Pr(Y^{(i)}|X^{(i)},\\hat{\\theta})\\right) \\\\ &amp;= \\sum_{i=1}^n\\log\\left(Pr(Y^{(i)}|X^{(i)},\\hat{\\theta})\\right) \\end{align*} \\] Since \\(\\log()\\) is a monotonic function, the maximums for both \\(l(\\hat{\\theta})\\) and \\(\\log(l(\\hat{\\theta}))\\) are the same. For this reason, we almost always exclusively use the log likelihood which is a summation rather than a product. For example, if we assume that \\(Pr(Y|X)\\) was a normal distribution of the form: \\[Pr(Y|X,\\theta)\\sim N\\left(X^T\\theta\\,\\,, \\,\\,1\\right)\\] Then the likelihood function is given by: \\[ \\begin{align*} l(\\hat{\\theta}) &amp;= \\prod_{i=1}^n Pr(Y^{(i)}|X^{(i)},\\hat{\\theta})\\\\ &amp;= \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(Y^{(i)}-X^{(i)}\\cdot\\hat{\\theta})^2} \\end{align*} \\] The best estimate \\(\\hat{\\theta}\\) is the one which maximizes this quantity. As before, we can switch to the log likelihood since the maximums will be the same for both \\(l(\\hat{\\theta})\\) and \\(\\log(l(\\hat{\\theta}))\\): \\[\\log(l(\\hat{\\theta})) = -\\frac{n\\log(2\\pi)}{2} + \\sum_{i=1}^n-\\frac{1}{2}\\left(Y^{(i)} - X^{(i)}\\cdot\\hat{\\theta}\\right)^2\\] First term \\(-\\frac{n\\log(2\\pi)}{2}\\) is a constant and \\(-\\frac{1}{2}\\) is a constant factor. Thus maximizing the log likelihood can be done simply by maximizing the quantity: \\[\\max_{\\hat{\\theta}} \\,\\,\\sum_{i=1}^n-\\left(Y^{(i)} - X^{(i)}\\cdot\\hat{\\theta}\\right)^2\\] If this quantity looks familiar, that is no coincidence; it is precisely the negative of the summation quantity in MSE for \\(f_{\\theta}(X)=X\\cdot \\theta\\) linear! \\[MSE = \\frac{1}{n}\\sum_{i=1}^n \\left(Y^{(i)} - X^{(i)}\\cdot\\hat{\\theta}\\right)^2\\] In particular: maximizing the log likelihood for \\(Pr(Y|X=x)\\) normal is equivalent to minimizing MSE for \\(f_{\\hat{\\theta}}\\) linear. 9.6 The additive error model and the bias-variance tradeoff We can conceptualizing \\(Y\\) as being made of two pieces of data: \\[Y = f(X) + \\epsilon\\] where \\(\\epsilon\\) is a random variable. Heuristically, \\(f(X)\\) is the best possible estimate for \\(Y\\) using \\(X\\), while \\(\\epsilon\\) is the inherent randomness of \\(Y\\) which cannot be predicted away. Since we have \\(f(x) = E(Y|X=x)\\), we must also have: \\[ \\begin{align*} E(Y|X=x) &amp;= E(f(X)+\\epsilon|X=x)\\\\ &amp;= E(f(X)|X=x) + E(\\epsilon|X=x)\\\\ &amp;= E(E(Y|X=x)) + E(\\epsilon|X=x)\\\\ &amp;= E(Y|X=x) + E(\\epsilon|X=x)\\\\ 0 &amp;=E(\\epsilon|X=x) \\end{align*} \\] In other words, the conditional mean of \\(\\epsilon\\) is always 0. Again, this represents the idea that \\(\\epsilon\\) is the randomness which cannot be predicted away; the information given by \\(X=x\\) gives us no information about \\(Y\\) beyond what is already given by \\(f(X)\\). This conceptualization \\(Y=f(X)+\\epsilon\\) is called the additive error model. The additive error model gives us a framework to study the prediction errors in our models. The prediction error of a model \\(\\hat{f}\\) is measured by the expected squared error (ESE): \\[ESE = E\\left[(Y-\\hat{f})^2\\right]\\] Let’s take a moment to appreciate this quantity; the ESE measures how how far-off the model \\(\\hat{f}\\) will be when trying to predict a general value \\(Y\\). Since \\(Y = f(X) + \\epsilon\\), we have: \\[ \\begin{align*} ESE &amp;= E\\left[(Y-\\hat{f}(X))^2\\right] \\\\ &amp;= E\\left[(f(X)+\\epsilon -\\hat{f}(X))^2\\right]\\\\ &amp;= ... \\\\ &amp;= \\left(f(X)-E[\\hat{f}]\\right)^2 + Var[\\hat{f}(X)] + Var[\\epsilon] \\end{align*} \\] The quantity \\(\\left(f(X)-E[\\hat{f}(X)]\\right)\\) is called the bias of \\(\\hat{f}\\) and measures how far the model function \\(\\hat{f}\\) differs from the ground truth function \\(f\\). The quantity \\(Var[\\hat{f}(X)]\\) is the variance of \\(\\hat{f}\\) and measures how wildly \\(\\hat{f}\\) tends to oscillate. The final term \\(Var[\\epsilon]\\) is the variance of \\(\\epsilon\\) and just measures the inherent randomness in \\(Y\\) which cannot be predicted away. Altogether, the equalities above say: \\[ESE = \\text{Bias}^2 + \\text{Variance} + Var(\\epsilon)\\] If we want to make the best possible predictions, we want to minimize \\(ESE\\). Now the random noise cannot be eliminated \\(Var(\\epsilon)\\) cannot be eliminated, but it is possible to lower both the bias and the variance by finding a model \\(\\hat{f}\\approx f\\). But since we don’t know what the true value of \\(Var(\\epsilon)\\) is, we don’t actually know if bias or variance are already low. Therein lies the rub: we always want to lower bias and variance but it isn’t always clear if this is possible. Furthermore, lowering bias will generally come at the cost of increased variance and vice versa. This phenomena is called the bias-variance tradeoff and is something we should always be mindful of when building and tuning models. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
