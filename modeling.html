<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Modeling | Cooking in R</title>
  <meta name="description" content="Chapter 9 Modeling | Cooking in R" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Modeling | Cooking in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Modeling | Cooking in R" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-import-and-storage.html"/>
<link rel="next" href="linear-regression-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="chapter" data-level="1" data-path="hello-world.html"><a href="hello-world.html"><i class="fa fa-check"></i><b>1</b> Hello, World!</a>
<ul>
<li class="chapter" data-level="1.1" data-path="hello-world.html"><a href="hello-world.html#installing-and-loading-packages"><i class="fa fa-check"></i><b>1.1</b> Installing and Loading Packages</a></li>
<li class="chapter" data-level="1.2" data-path="hello-world.html"><a href="hello-world.html#the-basics"><i class="fa fa-check"></i><b>1.2</b> The Basics</a></li>
<li class="chapter" data-level="1.3" data-path="hello-world.html"><a href="hello-world.html#data-types"><i class="fa fa-check"></i><b>1.3</b> Data Types</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="hello-world.html"><a href="hello-world.html#numerics"><i class="fa fa-check"></i><b>1.3.1</b> Numerics</a></li>
<li class="chapter" data-level="1.3.2" data-path="hello-world.html"><a href="hello-world.html#integers"><i class="fa fa-check"></i><b>1.3.2</b> Integers</a></li>
<li class="chapter" data-level="1.3.3" data-path="hello-world.html"><a href="hello-world.html#complex"><i class="fa fa-check"></i><b>1.3.3</b> Complex</a></li>
<li class="chapter" data-level="1.3.4" data-path="hello-world.html"><a href="hello-world.html#characters"><i class="fa fa-check"></i><b>1.3.4</b> Characters</a></li>
<li class="chapter" data-level="1.3.5" data-path="hello-world.html"><a href="hello-world.html#logicals"><i class="fa fa-check"></i><b>1.3.5</b> Logicals</a></li>
<li class="chapter" data-level="1.3.6" data-path="hello-world.html"><a href="hello-world.html#raws"><i class="fa fa-check"></i><b>1.3.6</b> Raws</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="hello-world.html"><a href="hello-world.html#operations"><i class="fa fa-check"></i><b>1.4</b> Operations</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="hello-world.html"><a href="hello-world.html#arithmetic-operations"><i class="fa fa-check"></i><b>1.4.1</b> Arithmetic Operations</a></li>
<li class="chapter" data-level="1.4.2" data-path="hello-world.html"><a href="hello-world.html#modular-division-and-integer-division"><i class="fa fa-check"></i><b>1.4.2</b> Modular Division and Integer Division</a></li>
<li class="chapter" data-level="1.4.3" data-path="hello-world.html"><a href="hello-world.html#relational-operators"><i class="fa fa-check"></i><b>1.4.3</b> Relational Operators</a></li>
<li class="chapter" data-level="1.4.4" data-path="hello-world.html"><a href="hello-world.html#logical-operators"><i class="fa fa-check"></i><b>1.4.4</b> Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="hello-world.html"><a href="hello-world.html#variable-and-function-assignment"><i class="fa fa-check"></i><b>1.5</b> Variable and Function Assignment</a></li>
<li class="chapter" data-level="1.6" data-path="hello-world.html"><a href="hello-world.html#control-flow"><i class="fa fa-check"></i><b>1.6</b> Control Flow</a></li>
<li class="chapter" data-level="1.7" data-path="hello-world.html"><a href="hello-world.html#loops"><i class="fa fa-check"></i><b>1.7</b> Loops</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="hello-world.html"><a href="hello-world.html#for-loops"><i class="fa fa-check"></i><b>1.7.1</b> “for” loops</a></li>
<li class="chapter" data-level="1.7.2" data-path="hello-world.html"><a href="hello-world.html#while-loops"><i class="fa fa-check"></i><b>1.7.2</b> “while” loops</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basic-data-structures.html"><a href="basic-data-structures.html"><i class="fa fa-check"></i><b>2</b> Basic Data Structures</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basic-data-structures.html"><a href="basic-data-structures.html#vectors"><i class="fa fa-check"></i><b>2.1</b> Vectors</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="basic-data-structures.html"><a href="basic-data-structures.html#building-vectors"><i class="fa fa-check"></i><b>2.1.1</b> Building Vectors</a></li>
<li class="chapter" data-level="2.1.2" data-path="basic-data-structures.html"><a href="basic-data-structures.html#accessing-vector-components"><i class="fa fa-check"></i><b>2.1.2</b> Accessing Vector Components</a></li>
<li class="chapter" data-level="2.1.3" data-path="basic-data-structures.html"><a href="basic-data-structures.html#vector-operations"><i class="fa fa-check"></i><b>2.1.3</b> Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basic-data-structures.html"><a href="basic-data-structures.html#lists"><i class="fa fa-check"></i><b>2.2</b> Lists</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="basic-data-structures.html"><a href="basic-data-structures.html#constructing-lists-and-accessing-elements"><i class="fa fa-check"></i><b>2.2.1</b> Constructing Lists and Accessing Elements</a></li>
<li class="chapter" data-level="2.2.2" data-path="basic-data-structures.html"><a href="basic-data-structures.html#look-ups"><i class="fa fa-check"></i><b>2.2.2</b> Look Ups</a></li>
<li class="chapter" data-level="2.2.3" data-path="basic-data-structures.html"><a href="basic-data-structures.html#infix-operators"><i class="fa fa-check"></i><b>2.2.3</b> Infix Operators</a></li>
<li class="chapter" data-level="2.2.4" data-path="basic-data-structures.html"><a href="basic-data-structures.html#inserting-and-removing-list-items"><i class="fa fa-check"></i><b>2.2.4</b> Inserting and Removing List Items</a></li>
<li class="chapter" data-level="2.2.5" data-path="basic-data-structures.html"><a href="basic-data-structures.html#nested-lists"><i class="fa fa-check"></i><b>2.2.5</b> Nested Lists</a></li>
<li class="chapter" data-level="2.2.6" data-path="basic-data-structures.html"><a href="basic-data-structures.html#lapply"><i class="fa fa-check"></i><b>2.2.6</b> lapply()</a></li>
<li class="chapter" data-level="2.2.7" data-path="basic-data-structures.html"><a href="basic-data-structures.html#joining-lists"><i class="fa fa-check"></i><b>2.2.7</b> Joining Lists</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="matrices.html"><a href="matrices.html"><i class="fa fa-check"></i><b>3</b> Matrices</a>
<ul>
<li class="chapter" data-level="3.1" data-path="matrices.html"><a href="matrices.html#constructing-matrices"><i class="fa fa-check"></i><b>3.1</b> Constructing Matrices</a></li>
<li class="chapter" data-level="3.2" data-path="matrices.html"><a href="matrices.html#special-matrices"><i class="fa fa-check"></i><b>3.2</b> Special Matrices</a></li>
<li class="chapter" data-level="3.3" data-path="matrices.html"><a href="matrices.html#accessing-matrix-entries"><i class="fa fa-check"></i><b>3.3</b> Accessing Matrix Entries</a></li>
<li class="chapter" data-level="3.4" data-path="matrices.html"><a href="matrices.html#adding-and-removing-rowscolumns"><i class="fa fa-check"></i><b>3.4</b> Adding and Removing Rows/Columns</a></li>
<li class="chapter" data-level="3.5" data-path="matrices.html"><a href="matrices.html#matrix-operations"><i class="fa fa-check"></i><b>3.5</b> Matrix Operations</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="matrices.html"><a href="matrices.html#addition"><i class="fa fa-check"></i><b>3.5.1</b> Addition</a></li>
<li class="chapter" data-level="3.5.2" data-path="matrices.html"><a href="matrices.html#scalar-multiplication"><i class="fa fa-check"></i><b>3.5.2</b> Scalar Multiplication</a></li>
<li class="chapter" data-level="3.5.3" data-path="matrices.html"><a href="matrices.html#hadamard-product"><i class="fa fa-check"></i><b>3.5.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="3.5.4" data-path="matrices.html"><a href="matrices.html#matrix-multiplication"><i class="fa fa-check"></i><b>3.5.4</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="3.5.5" data-path="matrices.html"><a href="matrices.html#aside-the-magic-of-matrix-multiplication"><i class="fa fa-check"></i><b>3.5.5</b> Aside: The Magic of Matrix Multiplication</a></li>
<li class="chapter" data-level="3.5.6" data-path="matrices.html"><a href="matrices.html#the-determinant"><i class="fa fa-check"></i><b>3.5.6</b> The Determinant</a></li>
<li class="chapter" data-level="3.5.7" data-path="matrices.html"><a href="matrices.html#inverses"><i class="fa fa-check"></i><b>3.5.7</b> Inverses</a></li>
<li class="chapter" data-level="3.5.8" data-path="matrices.html"><a href="matrices.html#transpose"><i class="fa fa-check"></i><b>3.5.8</b> Transpose</a></li>
<li class="chapter" data-level="3.5.9" data-path="matrices.html"><a href="matrices.html#kronecker-product"><i class="fa fa-check"></i><b>3.5.9</b> Kronecker Product</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-frames.html"><a href="data-frames.html"><i class="fa fa-check"></i><b>4</b> Data Frames</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data-frames.html"><a href="data-frames.html#summary-and-structure"><i class="fa fa-check"></i><b>4.1</b> Summary and Structure</a></li>
<li class="chapter" data-level="4.2" data-path="data-frames.html"><a href="data-frames.html#extracting-and-subsetting"><i class="fa fa-check"></i><b>4.2</b> Extracting and Subsetting</a></li>
<li class="chapter" data-level="4.3" data-path="data-frames.html"><a href="data-frames.html#editing-adding-and-removing-columns"><i class="fa fa-check"></i><b>4.3</b> Editing, Adding and Removing Columns</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html"><i class="fa fa-check"></i><b>5</b> Data visualization: ggplot2</a>
<ul>
<li class="chapter" data-level="5.1" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#grammar-of-graphics"><i class="fa fa-check"></i><b>5.1</b> Grammar of graphics</a></li>
<li class="chapter" data-level="5.2" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#loading-ggplot2"><i class="fa fa-check"></i><b>5.2</b> Loading ggplot2</a></li>
<li class="chapter" data-level="5.3" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#creating-a-ggplot"><i class="fa fa-check"></i><b>5.3</b> Creating a ggplot</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#ingredient-1-data-mapping-to-aesthetics"><i class="fa fa-check"></i><b>5.3.1</b> Ingredient 1: data mapping to aesthetics</a></li>
<li class="chapter" data-level="5.3.2" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#ingredient-2-layers-of-geometric-objects-equipped-with-mappings"><i class="fa fa-check"></i><b>5.3.2</b> Ingredient 2: layers of geometric objects equipped with mappings</a></li>
<li class="chapter" data-level="5.3.3" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#ingredient-3-scales"><i class="fa fa-check"></i><b>5.3.3</b> Ingredient 3: scales</a></li>
<li class="chapter" data-level="5.3.4" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#ingredient-4-a-coordinate-system"><i class="fa fa-check"></i><b>5.3.4</b> Ingredient 4: a coordinate system</a></li>
<li class="chapter" data-level="5.3.5" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#ingredient-5-a-facet-specification"><i class="fa fa-check"></i><b>5.3.5</b> Ingredient 5: a facet specification</a></li>
<li class="chapter" data-level="5.3.6" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#overwriting-data"><i class="fa fa-check"></i><b>5.3.6</b> Overwriting data</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#statistical-transformations"><i class="fa fa-check"></i><b>5.4</b> Statistical Transformations</a></li>
<li class="chapter" data-level="5.5" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#position"><i class="fa fa-check"></i><b>5.5</b> Position</a></li>
<li class="chapter" data-level="5.6" data-path="data-visualization-ggplot2.html"><a href="data-visualization-ggplot2.html#the-anatomy-of-a-graphic"><i class="fa fa-check"></i><b>5.6</b> The anatomy of a graphic</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html"><i class="fa fa-check"></i><b>6</b> Data transformation with <code>dplyr</code></a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#filtering-rows-with-filter"><i class="fa fa-check"></i><b>6.2</b> Filtering rows with <code>filter()</code></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#filtering-conditions"><i class="fa fa-check"></i><b>6.2.1</b> Filtering conditions</a></li>
<li class="chapter" data-level="6.2.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#dealing-with-missing-values"><i class="fa fa-check"></i><b>6.2.2</b> Dealing with missing values</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#arrange-rows-with-arrange"><i class="fa fa-check"></i><b>6.3</b> Arrange rows with <code>arrange()</code></a></li>
<li class="chapter" data-level="6.4" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#select-columns-with-select"><i class="fa fa-check"></i><b>6.4</b> Select columns with <code>select()</code></a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#helper-functions"><i class="fa fa-check"></i><b>6.4.1</b> Helper functions</a></li>
<li class="chapter" data-level="6.4.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#renaming-columns"><i class="fa fa-check"></i><b>6.4.2</b> Renaming columns</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#add-new-variables-with-mutate"><i class="fa fa-check"></i><b>6.5</b> Add new variables with <code>mutate()</code></a></li>
<li class="chapter" data-level="6.6" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#grouped-summaries-with-summarise-and-group_by"><i class="fa fa-check"></i><b>6.6</b> Grouped summaries with <code>summarise()</code> and <code>group_by()</code></a></li>
<li class="chapter" data-level="6.7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#an-example-analysis"><i class="fa fa-check"></i><b>6.7</b> An example analysis</a></li>
<li class="chapter" data-level="6.8" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#the-pipe-operator"><i class="fa fa-check"></i><b>6.8</b> The pipe operator <code>%&gt;%</code></a></li>
<li class="chapter" data-level="6.9" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#data-heirarchy-with-group_by"><i class="fa fa-check"></i><b>6.9</b> Data heirarchy with <code>group_by()</code></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>7</b> Exploratory Data Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#the-process-of-eda"><i class="fa fa-check"></i><b>7.1</b> The process of EDA</a></li>
<li class="chapter" data-level="7.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#questions"><i class="fa fa-check"></i><b>7.2</b> Questions</a></li>
<li class="chapter" data-level="7.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#questions-of-variation"><i class="fa fa-check"></i><b>7.3</b> Questions of variation</a></li>
<li class="chapter" data-level="7.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#questions-of-covariation"><i class="fa fa-check"></i><b>7.4</b> Questions of covariation</a></li>
<li class="chapter" data-level="7.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#missing-values"><i class="fa fa-check"></i><b>7.5</b> Missing values</a></li>
<li class="chapter" data-level="7.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#patterns-and-models"><i class="fa fa-check"></i><b>7.6</b> Patterns and models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-import-and-storage.html"><a href="data-import-and-storage.html"><i class="fa fa-check"></i><b>8</b> Data import and storage</a>
<ul>
<li class="chapter" data-level="8.1" data-path="data-import-and-storage.html"><a href="data-import-and-storage.html#tibbles"><i class="fa fa-check"></i><b>8.1</b> Tibbles</a></li>
<li class="chapter" data-level="8.2" data-path="data-import-and-storage.html"><a href="data-import-and-storage.html#data-import"><i class="fa fa-check"></i><b>8.2</b> Data import</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>9</b> Modeling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="modeling.html"><a href="modeling.html#a-concrete-example"><i class="fa fa-check"></i><b>9.1</b> A concrete example</a></li>
<li class="chapter" data-level="9.2" data-path="modeling.html"><a href="modeling.html#statistical-decision-theory"><i class="fa fa-check"></i><b>9.2</b> Statistical decision theory</a></li>
<li class="chapter" data-level="9.3" data-path="modeling.html"><a href="modeling.html#building-the-model"><i class="fa fa-check"></i><b>9.3</b> Building the model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="modeling.html"><a href="modeling.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>9.3.1</b> <span class="math inline">\(k\)</span>-nearest neighbors</a></li>
<li class="chapter" data-level="9.3.2" data-path="modeling.html"><a href="modeling.html#decision-trees"><i class="fa fa-check"></i><b>9.3.2</b> Decision trees</a></li>
<li class="chapter" data-level="9.3.3" data-path="modeling.html"><a href="modeling.html#model-based-approach"><i class="fa fa-check"></i><b>9.3.3</b> Model-based approach</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="modeling.html"><a href="modeling.html#minimizing-loss-functions"><i class="fa fa-check"></i><b>9.4</b> Minimizing loss functions</a></li>
<li class="chapter" data-level="9.5" data-path="modeling.html"><a href="modeling.html#maximizing-likelihood"><i class="fa fa-check"></i><b>9.5</b> Maximizing likelihood</a></li>
<li class="chapter" data-level="9.6" data-path="modeling.html"><a href="modeling.html#the-additive-error-model-and-the-bias-variance-tradeoff"><i class="fa fa-check"></i><b>9.6</b> The additive error model and the bias-variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>10</b> Linear Regression Models</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Cooking in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Modeling<a href="modeling.html#modeling" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Data usually comes with multiple variables and analyzing the covariation between these variables helps us understand the data. Model building is one method for analyzing this covariation. The model building process can be quite open-ended and freeform with an infinite number of design choices to be explored. We should formalize a basic framework to systematically study the model buidling process.</p>
<p>We start with an example of a situation highlighting the core elements of model building, then generalize this example to a more formal mathematical setting.</p>
<p><br />
</p>
<div id="a-concrete-example" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> A concrete example<a href="modeling.html#a-concrete-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As a concrete example, let’s suppose we wanted to study the relationship between a company’s marketing spend and their revenue over a given month. We can represent this scenario by letting <span class="math inline">\(X=(X_1,\ldots,X_k)\)</span> represent the amount of marketing spend across different marketing channels by a company. So we might have <span class="math inline">\(X_1\)</span> is TV ad spend, <span class="math inline">\(X_2\)</span> is social media spend, <span class="math inline">\(X_3\)</span> paid search spend, etc. We can let <span class="math inline">\(Y\)</span> be the revenue earned by that company.</p>
<p>It seems reasonable to believe that the more money a company spends on marketing, the more revenue they earn from sales. We want determine the exact nature of this relationship, so we can conceptualize revenue as a function of marketing spend: <span class="math inline">\(Y=f(X)\)</span>. The issue is that revenue figures are inherently random: some people will choose to buy (or not buy) a product on a whim. So it is unreasonable to expect that marketing spend <span class="math inline">\(X\)</span> can fully predict the exact value of revenue <span class="math inline">\(Y\)</span>. Just ask ourselves: if two companies have the same exact marketing spend values <span class="math inline">\(X=x\)</span>, can we guarantee they will always have the same revenue figures? The answer is probably “no!”</p>
<p>So if we can’t fully predict <span class="math inline">\(Y\)</span>, is there a related quantity we can hope to predict? It seems more reasonable to try and predict the expected value of <span class="math inline">\(Y\)</span> at given levels of marketing spend <span class="math inline">\(X=x\)</span>, which we call the <strong>conditional mean</strong> <span class="math inline">\(E(Y|X=x)\)</span>. Another quantity we can try to predict is the <em>conditional median</em> <span class="math inline">\(\text{median}(Y|X=x)\)</span>. So we can attempt to craft a function <span class="math inline">\(f\)</span> where <span class="math inline">\(f(x)=E(Y|X=x)\)</span> for every value of <span class="math inline">\(x\)</span>. This would allow us to predict a “best guess” estimate of revenue <span class="math inline">\(Y\)</span> at each possible spend level <span class="math inline">\(X=x\)</span>.</p>
<p>We want to explicity determine this function <span class="math inline">\(f\)</span> but this is not possible. In order to determine <span class="math inline">\(f\)</span> completely, we would need the <span class="math inline">\((X,Y)\)</span> data for every company that ever existed and every company that ever will exist! Let’s call this mythological data set containing every company the <em>population</em>. Since we can’t obtain the population data set, we can’t compute <span class="math inline">\(f\)</span> directly. Instead, we try to approximate it using an estimate <span class="math inline">\(\hat{f}\)</span>.</p>
<p>To construct the estimate <span class="math inline">\(\hat{f}\)</span>, what we can do is take a <strong>sample</strong> of data points <span class="math inline">\(\{(X^{(i)},Y^{(i)})\}_{i=1}^n\)</span> with each <span class="math inline">\((X^{(i)}, Y^{(i)})\)</span> representing a marketing spend value and a revenue figure. In our example, this might the marketing spend and revenue values from the same company each month over a period of 10 years. What we want to do is use the sample data as a proxy for the population data. How we build this is the main topic of discussion from this chapter onward.</p>
<p><br />
</p>
</div>
<div id="statistical-decision-theory" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Statistical decision theory<a href="modeling.html#statistical-decision-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We take the previous example and formalize it. Let <span class="math inline">\(X=(X_1,\ldots,X_k)\)</span> be a vector of <span class="math inline">\(k\)</span> variables and <span class="math inline">\(Y\)</span> a random variable which “depends” on <span class="math inline">\(X\)</span>. We want to study the exact nature of <span class="math inline">\(Y\)</span>’s dependency on <span class="math inline">\(X\)</span>. One approach is build a function <span class="math inline">\(f\)</span> which can construct <span class="math inline">\(Y\)</span> out of the data given by <span class="math inline">\(X\)</span>. In other words, we seek a function <span class="math inline">\(f\)</span> such that <span class="math inline">\(Y=f(X)\)</span>.</p>
<p>Unfortunately, since <span class="math inline">\(Y\)</span> is random, a function like this doesn’t exist. So we adjust our requirements and instead ask for an <span class="math inline">\(f\)</span> which predicts <span class="math inline">\(Y\)</span> as best as possible.</p>
<p>What do we mean by “best possible” prediction? Well there are multiple valid answers. One idea is to measure <span class="math inline">\(f\)</span>’s predictive power by measuring <span class="math inline">\(f\)</span>’s prediction error. A <strong>loss</strong> function <span class="math inline">\(L\)</span> is a functional <span class="math inline">\(L:\text{Maps}(\mathbb{R}^n,\mathbb{R})\to \mathbb{R}\)</span> where <span class="math inline">\(L(f)\)</span> represents the prediction error of <span class="math inline">\(f\)</span>. One such loss function is he <strong>expected squared error</strong> (ESE) given by: <span class="math display">\[
\begin{align*}
ESE(f) &amp;= E(Y-f(X))^2 \\
&amp;=\int(y-f(x))^2Pr(dx,dy) \\
&amp;=E_XE_{Y|X}\left([Y-f(X)]^2|X\right)
\end{align*}
\]</span> This quantity <span class="math inline">\(ESE\)</span> measures the average squared error made by <span class="math inline">\(f\)</span> when predicting the values of <span class="math inline">\(Y\)</span>. Since we want <span class="math inline">\(f(X)\)</span> to be the “best” possible prediction of <span class="math inline">\(Y\)</span>, <span class="math inline">\(f(X)\)</span> should minimize the <span class="math inline">\(ESE\)</span> quantity:</p>
<p><span class="math display">\[
f(x) = \min_{c} E_XE_{Y|X}\left([Y-c]^2|X=x\right)
\]</span> Think about what this equation is saying: for a fixed value <span class="math inline">\(X=x\)</span>, the quantity <span class="math inline">\(f(x)\)</span> should minimize the expected squared error <span class="math inline">\(E(Y-f(x))^2\)</span> in <span class="math inline">\(Y\)</span>. We actually know exactly what quantity <span class="math inline">\(f(x)\)</span> is, it’s just the (conditional) mean!</p>
<p><span class="math display">\[
f(X) = E(Y|X=x)
\]</span> To summarize: <span class="math inline">\(f(x)\)</span> should best predict <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span>. If we use <span class="math inline">\(ESE\)</span> as our standard for predictive power, then conditional mean <span class="math inline">\(f(X) = E(Y|X=x)\)</span> is the quantity that best predicts <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span>.</p>
<p><strong>Aside:</strong> This actually makes intuitive sense. In the absence of any information, the mean is always the best predictor in that it minimizes squared error. If we have <span class="math inline">\(Y\)</span> is dependent on <span class="math inline">\(X\)</span>, then once we control for <span class="math inline">\(X=x\)</span>, our best prediction for <span class="math inline">\(Y\)</span> should be the mean again <span class="math inline">\(E(Y|X=x)\)</span>.</p>
<p>Of course, <span class="math inline">\(ESE\)</span> is just one possible measure of prediction error. Another possible measure of prediction error is <strong>expected absolute error</strong> (EAE):</p>
<p><span class="math display">\[
EAE = E\big(\,|Y-f(X)| \,\big)
\]</span> The <span class="math inline">\(f\)</span> which minimizes this quantity is the conditional median:</p>
<p><span class="math display">\[
f(X) = \text{median}(Y|X=x)
\]</span></p>
<p>All-in-all: we want to study the dependency of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> by building a function <span class="math inline">\(f\)</span> which best predicts <span class="math inline">\(Y\)</span>. However, there are many different ways to quantify what “best” actually means and the quantity <span class="math inline">\(f(X)\)</span> can represent different statistical parameters.</p>
<p>In practice, the squared error ESE is usually preferred because of analytical properties like differentiability. That being said, the EAE is more robust to outliers.</p>
<p><br />
</p>
</div>
<div id="building-the-model" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Building the model<a href="modeling.html#building-the-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The function <span class="math inline">\(f(x)=E(Y|X=x)\)</span> offers the best predictions (in the sense of squared loss). We desire to explicitly know <span class="math inline">\(f\)</span>, but this requires knowing the entire distribution <span class="math inline">\(Pr(X,Y)\)</span> which is usually impossible to attain. The next best alternative is find an approximation <span class="math inline">\(\hat{f}\)</span> for the ground truth <span class="math inline">\(f\)</span>.</p>
<p>The idea is to sample a set of data points <span class="math inline">\(\{(X^{(i)},Y^{(i)})\}_{i=1}^n\)</span> from the distribution <span class="math inline">\(Pr(X,Y)\)</span> and use the sample as a <em>proxy</em> for the distribution. That is, we wish to estimate <span class="math inline">\(f(x) = E(Y|X=x)\)</span> from the sample. In a perfect world, we would have many data points <span class="math inline">\((x,Y)\)</span> for each fixed value of <span class="math inline">\(X=x\)</span> and we could average the <span class="math inline">\(Y\)</span> values across fixed <span class="math inline">\(X=x\)</span>. However, this is rarely the case and downright impossible for continuous <span class="math inline">\(X\)</span>.</p>
<div id="k-nearest-neighbors" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> <span class="math inline">\(k\)</span>-nearest neighbors<a href="modeling.html#k-nearest-neighbors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One approach: if we don’t have enough data points of a fixed <span class="math inline">\(x\)</span> value, why don’t we use data points with nearby <span class="math inline">\(x\)</span> values? For a given <span class="math inline">\(x\)</span> value, let <span class="math inline">\(N_k(x) = \{(X^{(i_1)}, Y^{(i_1)}),\ldots, (X^{(i_k)},Y^{(i_k)})\}\)</span> be the <span class="math inline">\(k\)</span> data points with <span class="math inline">\(X\)</span> values closest to <span class="math inline">\(x\)</span>. We call <span class="math inline">\(N_k(x)\)</span> the <span class="math inline">\(k\)</span>-nearest neighbors of <span class="math inline">\(x\)</span>. Then we make the approximation:</p>
<p><span class="math display">\[
\begin{align*}
f(X) &amp;= E(Y|X=x) \\
&amp; \approx E\left[N_k(x)\right] \\
&amp;= \frac{1}{k}\sum_{j=1}^k Y^{(i_j)}
\end{align*}
\]</span> Explicitly, the approximation we make is <span class="math inline">\(\hat{f}=E[N_k(x)]\)</span> and this model for <span class="math inline">\(f\)</span> is called the <span class="math inline">\(k\)</span>-nearest neighbors model.</p>
<p><br />
</p>
</div>
<div id="decision-trees" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Decision trees<a href="modeling.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <span class="math inline">\(k\)</span>-nearest neighbors approach motivates another approach to solving the problem. Since we want to approximate <span class="math inline">\(f(X) = E(Y|X=x)\)</span>, we can try to group similar data points with similar <span class="math inline">\(X=x\)</span> values together and take the group average. <span class="math inline">\(k\)</span>-nearest neighbors did this by defining “similar” to mean closest data points in Euclidean distance. What if we had a different method for defining “similar”?</p>
<p>Another approach is to identify clusters of data points together making splits along the <span class="math inline">\(X_1,\ldots, X_m\)</span> dimensions. In other words, we can draw rectangles in the <span class="math inline">\(X_1,\ldots,X_m\)</span> to group together data points in <span class="math inline">\(X\)</span> space by making splits along the coordinate <span class="math inline">\(X_i\)</span> axis. The optimal splits are chosen by greatest reduction to the variance in the <span class="math inline">\(Y^{(i)}\)</span>. The exact algorithm will be discussed later, but the main point is we are grouping together points in <span class="math inline">\(X\)</span>-space by optimizing some criterion.</p>
<p>After the groupings are computed, we average the <span class="math inline">\(Y^{(i)}\)</span> values in each cluster to get the estimate for <span class="math inline">\(E(Y|X=x)\)</span> we desire. This type of model is called a <strong>decision tree</strong>.</p>
<p><br />
</p>
</div>
<div id="model-based-approach" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Model-based approach<a href="modeling.html#model-based-approach" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another way to approximate <span class="math inline">\(f(x)\)</span> is to find a <span class="math inline">\(\hat{f}\)</span> which minimizes the prediction error over the sample. Recall that prediction error over the population is the expected squared error:</p>
<p><span class="math display">\[ESE(f) = E(Y-f(x))^2\]</span></p>
<p>The sample analogue of ESE is the <strong>mean squared error</strong> (MSE) given by:</p>
<p><span class="math display">\[
\begin{align*}
MSE(f) &amp;= E(Y^{(i)}-f(X^{(i)}))^2 \\
&amp;= \frac{1}{n}\sum_{i=1}^n\left(Y^{(i)}-f(X^{(i)})\right)^2
\end{align*}
\]</span> Since <span class="math inline">\(f\)</span> minimizes the ESE over the population, a good approximation for <span class="math inline">\(f\)</span> might be a function <span class="math inline">\(\hat{f}\)</span> which minimizes the MSE over the sample.</p>
<p>There is an issue with this approach: since we only have finitely many sample points, there are <em>infinitely many</em> functions which minimize MSE. Think about it: there exists infinitely many functions which cross through all the sample points, i.e. <span class="math inline">\(\hat{f}(X^{(i)})=Y^{(i)}\)</span> for all <span class="math inline">\(i\)</span>. Therefore, there exists infinitely many <span class="math inline">\(\hat{f}\)</span> for which <span class="math inline">\(MSE=0\)</span>.</p>
<p>In order to obtain a unique solution for <span class="math inline">\(\hat{f}\)</span>, we must impose a condition on the form of <span class="math inline">\(f\)</span> and <span class="math inline">\(\hat{f}\)</span>. For example:</p>
<ul>
<li><strong>Linear models</strong>: we assume <span class="math inline">\(f\)</span> is linear</li>
</ul>
<p><span class="math display">\[f(X) = X^T\theta = \theta_0+\theta_1X_1+\ldots+\theta_mX_m\]</span></p>
<ul>
<li><p><strong>Polynomial models</strong>: we assume <span class="math inline">\(f\)</span> is a polynomial in <span class="math inline">\(X_1,\ldots,X_n\)</span></p></li>
<li><p><strong>Generalized linear models</strong>: we assume <span class="math inline">\(f\)</span> is linear up to a link function</p></li>
</ul>
<p><span class="math display">\[f(X) = g\left(X^T\beta\right)\]</span></p>
<ul>
<li><strong>Generalized additive models</strong>: we assume <span class="math inline">\(f\)</span> is a linear combination of smooth functions up to a link function</li>
</ul>
<p><span class="math display">\[f(X) = g\left(\theta_0+f_1(X_1)+\ldots+f_m(X_m)\right)\]</span> Assuming <span class="math inline">\(f\)</span> must be of a specific form is called a <strong>model-based approach</strong> because we are modeling our <span class="math inline">\(f\)</span> after a specific function class.</p>
<p><br />
</p>
<p>Once we decide on a specific parametric form for <span class="math inline">\(f_{\theta}\)</span>, we can set about finding an estimate <span class="math inline">\(\hat{f}\)</span> by approximating the parameters <span class="math inline">\(\theta\)</span>. Again, we want our estimate <span class="math inline">\(\hat{f}\)</span> to minimize <span class="math inline">\(MSE\)</span> and this criterion is how we get estimates for the <span class="math inline">\(\theta\)</span> values.</p>
<p><br />
</p>
</div>
</div>
<div id="minimizing-loss-functions" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Minimizing loss functions<a href="modeling.html#minimizing-loss-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\hat{f}(x) = \hat{f}_{\theta}(x)\)</span> be a function parameterized by a set of values <span class="math inline">\(\theta = (\theta_1,\ldots,\theta_m)\)</span>. For example if <span class="math inline">\(\hat{f}(X) = X^T\beta\)</span> is linear, then <span class="math inline">\(\theta=\beta\)</span>. The objective is to find a specific <span class="math inline">\(\theta\)</span> such that the quantity:</p>
<p><span class="math display">\[MSE(\theta) = \frac{1}{n}\sum_{i=1}^n\left(Y^{(i)}-\hat{f}_{\theta}(X^{(i)})\right)^2\]</span></p>
<p>is minimized. Notice that we are treating <span class="math inline">\(MSE\)</span> as a function of <span class="math inline">\(\theta\)</span>. There are 2 ways we can attempt to minimize the <span class="math inline">\(MSE\)</span> quantity:</p>
<ol style="list-style-type: decimal">
<li>Analytic solution: do some calculus and clever linear algebra to solve for <span class="math inline">\(\theta\)</span> exactly.</li>
<li>Gradient descent: do an iterative computation that inches closer and closer to a solution.</li>
</ol>
<p>The first approach is generally only possible if <span class="math inline">\(\hat{f}_{\theta}\)</span> has a relatively simple form, e.g. a linear model. The second approach can always be applied, but can fail to converge to an optimal solution if <span class="math inline">\(MSE(\hat{f}_{\theta})\)</span> isn’t a convex function.</p>
<p>The concept of gradient descent actually follows a very simple calculus principle: the gradient points in the direction of steepest increase. Therefore, if we want to minimize a loss function <span class="math inline">\(L(\theta)\)</span>, we can start with a random guess <span class="math inline">\(\theta\)</span> and update our guess by moving in the opposite direction of the gradient:</p>
<p><span class="math display">\[\theta := \theta - \frac{dL}{d\theta}\]</span></p>
<p>After repeating this procedure, the hope is that we eventually arrive at a <span class="math inline">\(\theta\)</span> with an acceptably low loss value. As mentioned above, the main theoretical challenge for gradient descent is when the loss function <span class="math inline">\(L\)</span> is not convex, i.e. when <span class="math inline">\(L\)</span> has multiple local minimums.</p>
<p>There is also a number of computational challenges. Firstly, when the function <span class="math inline">\(f_{\theta}\)</span> is sufficiently complicated, the gradient <span class="math inline">\(\frac{dL}{d\theta}\)</span> can be extremely hard to compute. In these situations, it is often necessary to find a clever algorithm to compute the gradient. For example, neural networks have extremely complicated <span class="math inline">\(f_{\theta}\)</span> and the clever algorithm used to compute the gradient <span class="math inline">\(\frac{dL}{d\theta}\)</span> is called <strong>back propagation</strong>.</p>
<p>Secondly, gradient descent can become unstable if the step size along the gradient direction is too large. The naive solution to this problem is to take tiny steps during the descent, but this can lead to a very slow run time. A more sophisticated solution is to perform a <strong>line search</strong> for the optimal step size at each descent step.</p>
<p>Thirdly, gradient descent be quite inefficient even with an optimal step size. This is because the gradient can zigzag back-and-forth as it winds its way down to the minimum. For this reason, the gradient descent algorithm is often adjusted to provide a more “direct” approach towards the minimum. One such adjustment is <strong>conjugate gradient descent</strong> in which orthonormal basis vectors are used to adjust the gradient direction. Another approach is the <strong>Newton - Raphson method</strong> which attempts to solve the equation <span class="math inline">\(\frac{dL}{d\theta} = 0\)</span> via an iterative procedure</p>
<p><span class="math display">\[\theta:=\theta-\frac{L&#39;(\theta)}{L&#39;&#39;(\theta)}\]</span></p>
<p>(This is the same algorithm in Calculus 1 where students are asked to find the roots of a function <span class="math inline">\(f(x)\)</span> using tangent line approximations.)</p>
<p><br />
</p>
</div>
<div id="maximizing-likelihood" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Maximizing likelihood<a href="modeling.html#maximizing-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Minimizing lost functions isn’t the only game in town; we can also do <strong>maximum likelihood estimates</strong>. We have <span class="math inline">\(f(x) = E(Y|X=x)\)</span> is the “best possible prediction” for <span class="math inline">\(Y\)</span> using the information given by <span class="math inline">\(X=x\)</span>. Instead of trying to estimate the conditional mean <span class="math inline">\(E(Y|X=x)\)</span> directly, we instead do something more ambitious and try to estimate the entire conditional distribution <span class="math inline">\(Pr(Y|X=x)\)</span>.</p>
<p>Previously, we assumed that <span class="math inline">\(E(Y|X=x) = f_{\theta}(x)\)</span> had some specific model form, e.g. <span class="math inline">\(f_{\theta}=X^T\theta\)</span>. Now, we assume instead that the conditional distribution <span class="math inline">\(Pr(Y|X=x, \theta)\)</span> has a specific form, e.g. <span class="math inline">\(Pr(Y|X=x) \sim N(x^T\theta, \sigma^2)\)</span>. Finding a good approximation for <span class="math inline">\(Pr(Y|X=x)\)</span> now reduces to finding a good estimate for <span class="math inline">\(\theta\)</span>.</p>
<p>For any given value <span class="math inline">\(\hat{\theta}\)</span>, define the <strong>likelihood</strong> of <span class="math inline">\(\hat{\theta}\)</span> given <span class="math inline">\((X,Y)\)</span> as the quantity:</p>
<p><span class="math display">\[l(\hat{\theta} | X,Y) = Pr(Y|X,\hat{\theta})\]</span> In other words, the likelihood of <span class="math inline">\(\hat{\theta}\)</span> is the probability of observing the values <span class="math inline">\((X,Y)\)</span> if <span class="math inline">\(\hat{\theta}\)</span> was the actual parameter of the distribution <span class="math inline">\(Pr(Y|X)\)</span>.</p>
<p>The <strong>principle of maximum likelihood</strong> espouses that the <span class="math inline">\(\hat{\theta}\)</span> which best estimates <span class="math inline">\(\theta\)</span> is the one with the greatest likelihood. Specifically:</p>
<p><span class="math display">\[
\begin{align*}
\hat{\theta} &amp;:= \max_{\hat{\theta}} \,\, l(\hat{\theta}|X^{(1)},\ldots,X^{(n)},Y^{(1)},\ldots,Y^{(n)}) \\
&amp;= \max_{\hat{\theta}} \,\,Pr(Y^{(1)},\ldots,Y^{(n)}|X^{(1)},\ldots,X^{(n)},\hat{\theta})\\
&amp;= \max_{\hat{\theta}} \,\,\prod_{i=1}^n Pr(Y^{(i)}|X^{(i)},\hat{\theta})
\end{align*}
\]</span></p>
<p>In other words, <span class="math inline">\(\hat{\theta}\)</span> should be the parameter(s) for which the observed sample is most probable. Note that the we are able to split the probability into a product because we assume the each data point is drawn independently during the sampling process.</p>
<p>We can convert the product into a sum by feeding the likelihood <span class="math inline">\(l(\hat{\theta})\)</span> into the <span class="math inline">\(\log()\)</span> function. If we do this, we get:</p>
<p><span class="math display">\[
\begin{align*}
\log(l(\hat{\theta})) &amp;= \log\left(\prod_{i=1}^n Pr(Y^{(i)}|X^{(i)},\hat{\theta})\right) \\
&amp;= \sum_{i=1}^n\log\left(Pr(Y^{(i)}|X^{(i)},\hat{\theta})\right)
\end{align*}
\]</span></p>
<p>Since <span class="math inline">\(\log()\)</span> is a monotonic function, the maximums for both <span class="math inline">\(l(\hat{\theta})\)</span> and <span class="math inline">\(\log(l(\hat{\theta}))\)</span> <em>are the same</em>. For this reason, we almost always exclusively use the <strong>log likelihood</strong> which is a summation rather than a product.</p>
<p>For example, if we assume that <span class="math inline">\(Pr(Y|X)\)</span> was a normal distribution of the form:</p>
<p><span class="math display">\[Pr(Y|X,\theta)\sim N\left(X^T\theta\,\,, \,\,1\right)\]</span> Then the likelihood function is given by:</p>
<p><span class="math display">\[
\begin{align*}
l(\hat{\theta}) &amp;= \prod_{i=1}^n Pr(Y^{(i)}|X^{(i)},\hat{\theta})\\
&amp;= \prod_{i=1}^n\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(Y^{(i)}-X^{(i)}\cdot\hat{\theta})^2}
\end{align*}
\]</span></p>
<p>The best estimate <span class="math inline">\(\hat{\theta}\)</span> is the one which maximizes this quantity. As before, we can switch to the log likelihood since the maximums will be the same for both <span class="math inline">\(l(\hat{\theta})\)</span> and <span class="math inline">\(\log(l(\hat{\theta}))\)</span>:</p>
<p><span class="math display">\[\log(l(\hat{\theta})) = -\frac{n\log(2\pi)}{2} + \sum_{i=1}^n-\frac{1}{2}\left(Y^{(i)} - X^{(i)}\cdot\hat{\theta}\right)^2\]</span></p>
<p>First term <span class="math inline">\(-\frac{n\log(2\pi)}{2}\)</span> is a constant and <span class="math inline">\(-\frac{1}{2}\)</span> is a constant factor. Thus maximizing the log likelihood can be done simply by maximizing the quantity:</p>
<p><span class="math display">\[\max_{\hat{\theta}} \,\,\sum_{i=1}^n-\left(Y^{(i)} - X^{(i)}\cdot\hat{\theta}\right)^2\]</span></p>
<p>If this quantity looks familiar, that is no coincidence; it is precisely the negative of the summation quantity in MSE for <span class="math inline">\(f_{\theta}(X)=X\cdot \theta\)</span> linear!</p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum_{i=1}^n \left(Y^{(i)} - X^{(i)}\cdot\hat{\theta}\right)^2\]</span></p>
<p>In particular: maximizing the log likelihood for <span class="math inline">\(Pr(Y|X=x)\)</span> normal <em>is equivalent</em> to minimizing MSE for <span class="math inline">\(f_{\hat{\theta}}\)</span> linear.</p>
<p><br />
</p>
</div>
<div id="the-additive-error-model-and-the-bias-variance-tradeoff" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> The additive error model and the bias-variance tradeoff<a href="modeling.html#the-additive-error-model-and-the-bias-variance-tradeoff" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><br />
</p>
<p>We can conceptualizing <span class="math inline">\(Y\)</span> as being made of two pieces of data:</p>
<p><span class="math display">\[Y = f(X) + \epsilon\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is a random variable. Heuristically, <span class="math inline">\(f(X)\)</span> is the best possible estimate for <span class="math inline">\(Y\)</span> using <span class="math inline">\(X\)</span>, while <span class="math inline">\(\epsilon\)</span> is the inherent randomness of <span class="math inline">\(Y\)</span> which cannot be predicted away.</p>
<p>Since we have <span class="math inline">\(f(x) = E(Y|X=x)\)</span>, we must also have:</p>
<p><span class="math display">\[
\begin{align*}
E(Y|X=x) &amp;= E(f(X)+\epsilon|X=x)\\
&amp;= E(f(X)|X=x) + E(\epsilon|X=x)\\
&amp;= E(E(Y|X=x)) + E(\epsilon|X=x)\\
&amp;= E(Y|X=x) + E(\epsilon|X=x)\\
0 &amp;=E(\epsilon|X=x)
\end{align*}
\]</span> In other words, the conditional mean of <span class="math inline">\(\epsilon\)</span> is always 0. Again, this represents the idea that <span class="math inline">\(\epsilon\)</span> is the randomness which cannot be predicted away; the information given by <span class="math inline">\(X=x\)</span> gives us no information about <span class="math inline">\(Y\)</span> beyond what is already given by <span class="math inline">\(f(X)\)</span>.</p>
<p>This conceptualization <span class="math inline">\(Y=f(X)+\epsilon\)</span> is called the <strong>additive error model</strong>. The additive error model gives us a framework to study the prediction errors in our models. The prediction error of a model <span class="math inline">\(\hat{f}\)</span> is measured by the expected squared error (ESE):</p>
<p><span class="math display">\[ESE = E\left[(Y-\hat{f})^2\right]\]</span></p>
<p>Let’s take a moment to appreciate this quantity; the ESE measures how how far-off the model <span class="math inline">\(\hat{f}\)</span> will be when trying to predict a general value <span class="math inline">\(Y\)</span>. Since <span class="math inline">\(Y = f(X) + \epsilon\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
ESE &amp;= E\left[(Y-\hat{f}(X))^2\right] \\
&amp;= E\left[(f(X)+\epsilon -\hat{f}(X))^2\right]\\
&amp;= ... \\
&amp;= \left(f(X)-E[\hat{f}]\right)^2 + Var[\hat{f}(X)] + Var[\epsilon]
\end{align}
\]</span></p>
<p>The quantity <span class="math inline">\(\left(f(X)-E[\hat{f}(X)]\right)\)</span> is called the <strong>bias</strong> of <span class="math inline">\(\hat{f}\)</span> and measures how far the model function <span class="math inline">\(\hat{f}\)</span> differs from the ground truth function <span class="math inline">\(f\)</span>. The quantity <span class="math inline">\(Var[\hat{f}(X)]\)</span> is the <strong>variance</strong> of <span class="math inline">\(\hat{f}\)</span> and measures how wildly <span class="math inline">\(\hat{f}\)</span> tends to oscillate. The final term <span class="math inline">\(Var[\epsilon]\)</span> is the variance of <span class="math inline">\(\epsilon\)</span> and just measures the inherent randomness in <span class="math inline">\(Y\)</span> which cannot be predicted away. Altogether, the equalities above say:</p>
<p><span class="math display">\[ESE = \text{Bias}^2 + \text{Variance} + Var(\epsilon)\]</span>
If we want to make the best possible predictions, we want to minimize <span class="math inline">\(ESE\)</span>. Now the random noise cannot be eliminated <span class="math inline">\(Var(\epsilon)\)</span> cannot be eliminated, but it is possible to lower both the bias and the variance by finding a model <span class="math inline">\(\hat{f}\approx f\)</span>. But since we don’t know what the true value of <span class="math inline">\(Var(\epsilon)\)</span> is, we don’t actually know if bias or variance are already low. Therein lies the rub: we always want to lower bias and variance but it isn’t always clear if this is possible.</p>
<p>Furthermore, lowering bias will generally come at the cost of increased variance and vice versa. This phenomena is called the <strong>bias-variance tradeoff</strong> and is something we should always be mindful of when building and tuning models.</p>
<p><br />
</p>
<hr />
<p><br />
</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-import-and-storage.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/rmd_src/09_modeling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
